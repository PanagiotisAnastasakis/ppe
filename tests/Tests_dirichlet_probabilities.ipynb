{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.path.abspath(os.path.dirname(\"Tests_dirichlet_probabilities\"))\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from ppe import Dirichlet, PPEProbabilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as scs\n",
    "import pymc as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some simple checks regarding the Dirichlet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.7239618\n",
      "19.97801\n",
      "True\n",
      "-----------------\n",
      "-2.7055316\n",
      "24.51511\n",
      "----------------\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "incorrect_probs = [np.array([0.5,0.1,0.2,0.3])]\n",
    "expert_probs = [np.array([0.2,0.15,0.25,0.4])]\n",
    "\n",
    "dir_1 = Dirichlet(alpha=1, J=1)\n",
    "\n",
    "## Gives an error, as it should\n",
    "\n",
    "##print(dir_1.alpha_mle(incorrect_probs, expert_probs))\n",
    "\n",
    "\n",
    "probs = [np.array([0.3,0.05,0.2,0.45])]\n",
    "expert_probs = [np.array([0.2,0.15,0.25,0.4])]\n",
    "\n",
    "\n",
    "#print(dir_1.pdf(probs, expert_probs))\n",
    "print(dir_1.llik(probs, expert_probs, index=0))\n",
    "print(dir_1.alpha_mle(probs, expert_probs))\n",
    "\n",
    "\n",
    "dir_2 = Dirichlet(None, J=1)\n",
    "dir_3 = Dirichlet(dir_1.alpha_mle(probs, expert_probs), J=1)\n",
    "\n",
    "##checking that if alpha is None, then it is computed based on dirichlet mle\n",
    "\n",
    "print(dir_2.llik(probs, expert_probs, index=0) == dir_3.llik(probs, expert_probs, index=0))\n",
    "\n",
    "print(\"-----------------\")\n",
    "\n",
    "sample_incorrect_probs = [[0.3,0.05,0.2,0.45], [0.35,0.05,0.2,0.45]]\n",
    "sample_expert_probs = [[0.2,0.15,0.25,0.4], [0.2,0.15,0.25,0.4]]\n",
    "\n",
    "## Gives an error\n",
    "\n",
    "## print(dir_1.sum_llik(sample_incorrect_probs, sample_expert_probs))\n",
    "\n",
    "\n",
    "\n",
    "sample_probs = [np.array([0.3,0.05,0.2,0.45]), np.array([0.15,0.25,0.5,0.1])]\n",
    "sample_expert_probs = [np.array([0.2,0.15,0.25,0.4]), np.array([0.1,0.2,0.5,0.2])]\n",
    "\n",
    "dir_4 = Dirichlet(alpha=1, J=2)\n",
    "\n",
    "\n",
    "print(dir_4.sum_llik(sample_probs, sample_expert_probs))\n",
    "print(dir_4.alpha_mle(sample_probs, sample_expert_probs))\n",
    "\n",
    "print(\"----------------\")\n",
    "\n",
    "\n",
    "dir_5 = Dirichlet(dir_4.alpha_mle(sample_probs, sample_expert_probs), J=2)\n",
    "dir_6 = Dirichlet(None, J=2)\n",
    "\n",
    "##checking that if alpha is None, then it is computed based on dirichlet mle\n",
    "\n",
    "print(dir_5.sum_llik(sample_probs, sample_expert_probs) == dir_6.sum_llik(sample_probs, sample_expert_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8049607\n",
      "4.0574403\n"
     ]
    }
   ],
   "source": [
    "## Checking that having different probability dimensions is ok (necessary for having different number of partitions)\n",
    "\n",
    "\n",
    "\n",
    "sample_probs = [np.array([0.3,0.05,0.2,0.45]), np.array([0.3, 0.25, 0.45])] ## 4 and 3 probabilities (partitions)\n",
    "sample_expert_probs = [np.array([0.2,0.15,0.25,0.4]), np.array([0.2, 0.7, 0.1])]\n",
    "\n",
    "dir_1 = Dirichlet(None, J=2)\n",
    "\n",
    "\n",
    "print(dir_1.sum_llik(sample_probs, sample_expert_probs))\n",
    "print(dir_1.alpha_mle(sample_probs, sample_expert_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running some tests to ensure that the two classes work as expected\n",
    "\n",
    "## Discrete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2.]\n",
      "[array([0.3, 0.1, 0.6]), array([0.4, 0.4, 0.2]), array([0.5, 0.3, 0.2])]\n"
     ]
    }
   ],
   "source": [
    "############### Getting the data from a folder ###############\n",
    "\n",
    "probs_1 = np.array([[0, 1, 2], [0.5, 0.3, 0.2]]).T\n",
    "probs_2 = np.array([[0, 1, 2], [0.4, 0.4, 0.2]]).T\n",
    "probs_3 = np.array([[0, 1, 2], [0.3, 0.1, 0.6]]).T\n",
    "\n",
    "# Folder path to store CSV files\n",
    "folder_path = current_dir\n",
    "\n",
    "probs_1 = pd.DataFrame(probs_1)\n",
    "probs_2 = pd.DataFrame(probs_2)\n",
    "probs_3 = pd.DataFrame(probs_3)\n",
    "\n",
    "probs_1.to_csv(folder_path + \"/probs_1.csv\")\n",
    "probs_2.to_csv(folder_path + \"/probs_2.csv\")\n",
    "probs_3.to_csv(folder_path + \"/probs_3.csv\")\n",
    "\n",
    "prob_class = PPEProbabilities(target_type=\"discrete\", path=True)\n",
    "\n",
    "\n",
    "partitions, expert_probs = prob_class.get_expert_data(folder_path)\n",
    "\n",
    "print(partitions)\n",
    "print(expert_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2.]\n",
      "[array([0.5, 0.3, 0.2]), array([0.4, 0.4, 0.2]), array([0.3, 0.1, 0.6])]\n"
     ]
    }
   ],
   "source": [
    "############### Getting the data from a file ###############\n",
    "\n",
    "probs_file = np.array([[0, 1, 2], [0.5, 0.3, 0.2], [0.4, 0.4, 0.2], [0.3, 0.1, 0.6]]).T\n",
    "\n",
    "# Path to store the CSV file\n",
    "\n",
    "probs_file = pd.DataFrame(probs_file)\n",
    "\n",
    "probs_file.to_csv(current_dir + '/probs_file.csv')\n",
    "\n",
    "prob_class = PPEProbabilities(target_type=\"discrete\", path=True)\n",
    "\n",
    "\n",
    "partitions, expert_probs = prob_class.get_expert_data(current_dir + '/probs_file.csv')\n",
    "\n",
    "print(partitions)\n",
    "print(expert_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2.]\n",
      "[array([0.5, 0.3, 0.2]), array([0.4, 0.4, 0.2]), array([0.3, 0.1, 0.6])]\n"
     ]
    }
   ],
   "source": [
    "############### Feeding the data directly ###############\n",
    "\n",
    "\n",
    "probs = np.array([[0, 1, 2], [0.5, 0.3, 0.2], [0.4, 0.4, 0.2], [0.3, 0.1, 0.6]]).T\n",
    "\n",
    "prob_class = PPEProbabilities(target_type=\"discrete\", path=False)\n",
    "\n",
    "\n",
    "partitions, expert_probs = prob_class.get_expert_data(probs)\n",
    "\n",
    "print(partitions)\n",
    "print(expert_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "[array([0.3, 0.7]), array([0.6, 0.4])]\n",
      "[array([0.2845, 0.7155]), array([0.609, 0.391])]\n",
      "------------\n",
      "1338.6366\n",
      "5.83667\n",
      "1.8725219\n"
     ]
    }
   ],
   "source": [
    "######## Test where we input samples from a simple prior predictive distribution and get model probabilities for the partitions ########\n",
    "\n",
    "elicited_data_discrete = np.array([[0,1],[0.3,0.7],[0.6,0.4]]).T\n",
    "\n",
    "\n",
    "cov_set_1 = np.random.binomial(n = 1, p = 0.7, size = 2000)\n",
    "cov_set_2 = np.random.binomial(n = 1, p = 0.4, size = 2000)\n",
    "\n",
    "samples = np.vstack((cov_set_1, cov_set_2)).T\n",
    "\n",
    "prob_class = PPEProbabilities(target_type=\"discrete\", path=False)\n",
    "\n",
    "\n",
    "partitions, expert_probs = prob_class.get_expert_data(elicited_data_discrete)\n",
    "\n",
    "model_probs = prob_class.ppd_probs(samples, partitions)\n",
    "\n",
    "print(partitions)\n",
    "print(expert_probs)\n",
    "print(model_probs)\n",
    "print(\"------------\")\n",
    "\n",
    "## Feeding these probabilities to dirichlet\n",
    "\n",
    "dir = Dirichlet(None, J = 2)\n",
    "\n",
    "print(dir.alpha_mle(model_probs, expert_probs))  ## very high alpha, which makes sense since we used the \"expert\" probabilities to sample\n",
    "\n",
    "print(dir.sum_llik(model_probs, expert_probs))\n",
    "\n",
    "dir_2 = Dirichlet(10, J = 2) ## trying fixed alpha\n",
    "\n",
    "print(dir_2.sum_llik(model_probs, expert_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.5]\n",
      " [1.  0.3]\n",
      " [2.  0.2]]\n",
      "[[  0. 100.]\n",
      " [100. 150.]\n",
      " [150. 200.]\n",
      " [200. 300.]]\n",
      "[[  0. 150.]\n",
      " [150. 200.]\n",
      " [200. 300.]]\n",
      "[[  0. 100.]\n",
      " [100. 200.]\n",
      " [200. 300.]]\n",
      "[array([0.3, 0.1, 0.6]), array([0.3, 0.1, 0.4, 0.2]), array([0.4, 0.4, 0.2]), array([0.5, 0.3, 0.2])]\n"
     ]
    }
   ],
   "source": [
    "############### Getting the data from a folder ###############\n",
    "\n",
    "probs_1 = np.array([[0, 100, 200], [100, 200, 300], [0.5, 0.3, 0.2]]).T  ## partitions (0,100), (100, 200), (200, 300)\n",
    "probs_2 = np.array([[0, 150, 200], [150, 200, 300], [0.4, 0.4, 0.2]]).T  ## partitions (0,150), (150, 200), (200, 300)\n",
    "probs_3 = np.array([[0, 100, 150, 200], [100, 150, 200, 300], [0.3, 0.1, 0.4, 0.2]]).T   ## Different number of partitions here! partitions (0,100), (100, 150), (150, 200), (200, 300)\n",
    "\n",
    "# Folder path to store CSV files\n",
    "folder_path = current_dir\n",
    "\n",
    "probs_1 = pd.DataFrame(probs_1)\n",
    "probs_2 = pd.DataFrame(probs_2)\n",
    "probs_3 = pd.DataFrame(probs_3)\n",
    "\n",
    "probs_1.to_csv(folder_path + \"/probs_1.csv\")\n",
    "probs_2.to_csv(folder_path + \"/probs_2.csv\")\n",
    "probs_3.to_csv(folder_path + \"/probs_3.csv\")\n",
    "\n",
    "prob_class = PPEProbabilities(target_type=\"continuous\", path=True)\n",
    "\n",
    "\n",
    "partitions, expert_probs = prob_class.get_expert_data(folder_path)\n",
    "\n",
    "for partition in partitions:\n",
    "    print(partition)\n",
    "\n",
    "print(expert_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0. 100.]\n",
      " [100. 200.]\n",
      " [200. 300.]]\n",
      "[[  0. 150.]\n",
      " [150. 200.]\n",
      " [200. 300.]]\n",
      "[[  0. 100.]\n",
      " [100. 150.]\n",
      " [150. 200.]\n",
      " [200. 300.]]\n",
      "[array([0.5, 0.3, 0.2]), array([0.4, 0.4, 0.2]), array([0.3, 0.1, 0.4, 0.2])]\n"
     ]
    }
   ],
   "source": [
    "############### Feeding the data directly ###############\n",
    "\n",
    "probs_1 = np.array([[0, 100, 200], [100, 200, 300], [0.5, 0.3, 0.2]]).T\n",
    "probs_2 = np.array([[0, 150, 200], [150, 200, 300], [0.4, 0.4, 0.2]]).T\n",
    "probs_3 = np.array([[0, 100, 150, 200], [100, 150, 200, 300], [0.3, 0.1, 0.4, 0.2]]).T   ## Different number of partitions here!\n",
    "\n",
    "probs = [probs_1, probs_2, probs_3]\n",
    "\n",
    "prob_class = PPEProbabilities(target_type=\"continuous\", path=False)\n",
    "\n",
    "\n",
    "partitions, expert_probs = prob_class.get_expert_data(probs)\n",
    "\n",
    "for partition in partitions:\n",
    "    print(partition)\n",
    "\n",
    "print(expert_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[  0., 100.],\n",
      "       [100., 200.],\n",
      "       [200., 300.]]), array([[  0., 150.],\n",
      "       [150., 200.],\n",
      "       [200., 300.]]), array([[  0., 100.],\n",
      "       [100., 150.],\n",
      "       [150., 200.],\n",
      "       [200., 300.]])]\n",
      "[array([0.5, 0.3, 0.2]), array([0.4, 0.4, 0.2]), array([0.3, 0.1, 0.4, 0.2])]\n",
      "[array([0.7726, 0.2259, 0.0015]), array([0.5042, 0.419 , 0.0768]), array([0.0027, 0.0738, 0.4179, 0.5056])]\n",
      "------------\n",
      "4.4900465\n",
      "-6.5192842\n",
      "-6.963935\n"
     ]
    }
   ],
   "source": [
    "######## Test where we input samples from a simple prior predictive distribution and get model probabilities for the partitions ########\n",
    "\n",
    "probs_1 = np.array([[0, 100, 200], [100, 200, 300], [0.5, 0.3, 0.2]]).T\n",
    "probs_2 = np.array([[0, 150, 200], [150, 200, 300], [0.4, 0.4, 0.2]]).T\n",
    "probs_3 = np.array([[0, 100, 150, 200], [100, 150, 200, 300], [0.3, 0.1, 0.4, 0.2]]).T   ## Different number of partitions here!\n",
    "\n",
    "probs = [probs_1, probs_2, probs_3]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "samples_1 = np.random.normal(loc = 66, scale = 45, size = 10000)\n",
    "samples_2 = np.random.normal(loc = 150, scale = 35, size = 10000)\n",
    "samples_3 = np.random.normal(loc = 200, scale = 35, size = 10000)\n",
    "\n",
    "samples = np.vstack((samples_1, samples_2, samples_3)).T\n",
    "\n",
    "\n",
    "prob_class = PPEProbabilities(target_type=\"continuous\", path=False)\n",
    "\n",
    "\n",
    "partitions, expert_probs = prob_class.get_expert_data(probs)\n",
    "\n",
    "model_probs = prob_class.ppd_probs(samples, partitions)\n",
    "\n",
    "print(partitions)\n",
    "print(expert_probs)\n",
    "print(model_probs)\n",
    "print(\"------------\")\n",
    "\n",
    "\n",
    "\n",
    "dir = Dirichlet(None, J=3)\n",
    "\n",
    "print(dir.alpha_mle(model_probs, expert_probs))  ## low alpha, which makes sense since we used a \"random\" distribution to sample, loosely following the expert distribution\n",
    "\n",
    "print(dir.sum_llik(model_probs, expert_probs))\n",
    "\n",
    "dir_2 = Dirichlet(10, J=3) ## trying fixed alpha\n",
    "\n",
    "print(dir_2.sum_llik(model_probs, expert_probs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running some more complicated examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1 (from paper): Univariate Gaussian\n",
    "\n",
    "We assume $ Y \\sim \\mathcal{N}(\\theta, \\sigma)$, with $\\theta \\sim \\frac{1}{2} \\mathcal{N}(\\mu_1, \\sigma_1) + \\frac{1}{2} \\mathcal{N}(\\mu_2, \\sigma_2)$. Then, we have the hyperparameter vector $\\pmb{\\lambda} = [\\mu_1, \\mu_2, \\sigma, \\sigma_1, \\sigma_2]$. Also, for $ A = (a,b] $, we know that \n",
    "\n",
    "$$\\mathbb{P}_{A|\\pmb{\\lambda}} = \\sum_{k=1}^{2} \\Big( \\frac{1}{2} \\Phi \\Big((b - \\mu_k)/\\sqrt{\\sigma^2 + \\sigma_k^2} \\Big) - \\frac{1}{2} \\Phi \\Big((a - \\mu_k)/\\sqrt{\\sigma^2 + \\sigma_k^2} \\Big) \\Big)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.2, 0.3, 0.3, 0.2]), array([0.45, 0.55]), array([0.35, 0.3 , 0.35])]\n",
      "[array([0.2893248, 0.2106752, 0.2106752, 0.2893248]), array([0.5, 0.5]), array([0.2893248, 0.4213504, 0.2893248])]\n"
     ]
    }
   ],
   "source": [
    "mu_1 = 1\n",
    "mu_2 = -1\n",
    "sigma = sigma_1 = sigma_2 = 1\n",
    "\n",
    "def get_gaussian_probs(partition):\n",
    "\n",
    "    p1 = 0.5*(scs.norm.cdf((partition[1] - mu_1)/np.sqrt(sigma**2 + sigma_1**2)) - scs.norm.cdf((partition[0] - mu_1)/np.sqrt(sigma**2 + sigma_1**2)))\n",
    "    p2 = 0.5*(scs.norm.cdf((partition[1] - mu_2)/np.sqrt(sigma**2 + sigma_2**2)) - scs.norm.cdf((partition[0] - mu_2)/np.sqrt(sigma**2 + sigma_2**2)))\n",
    "\n",
    "    return p1 + p2\n",
    "\n",
    "\n",
    "## expert probabilities\n",
    "\n",
    "probs_1 = np.array([[-35,-1,0,1], [-1,0,1,35], [0.2, 0.3, 0.3, 0.2]]).T ## partitions (-35, -1), (-1, 0), (0, 1), (1, 35)\n",
    "probs_2 = np.array([[-35,0], [0,35], [0.45, 0.55]]).T ## partitions (-35, 0), (0, 35)\n",
    "probs_3 = np.array([[-35,-1,1], [-1,1,35], [0.35, 0.3, 0.35]]).T ## partitions (-35, -1), (-1, 1), (1, 35)\n",
    "\n",
    "\n",
    "\n",
    "probs = [probs_1, probs_2, probs_3]\n",
    "\n",
    "prob_class = PPEProbabilities(target_type=\"continuous\", path=False)\n",
    "\n",
    "\n",
    "partitions, expert_probs = prob_class.get_expert_data(probs)\n",
    "\n",
    "\n",
    "model_probs = []\n",
    "\n",
    "for i in range(len(partitions)):\n",
    "\n",
    "    probs = np.array([get_gaussian_probs(partition) for partition in partitions[i]])\n",
    "\n",
    "    model_probs.append(probs)\n",
    "\n",
    "\n",
    "print(expert_probs)\n",
    "print(model_probs) ## quite close to the true ones with all partition sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha as computed based on the given probabilities: 29.208767\n",
      "Log likelihood when alpha is not given (computed internally): 6.697336\n",
      "Log likelihood when alpha = 10 (fixed): 5.309321\n"
     ]
    }
   ],
   "source": [
    "dir = Dirichlet(None, J=3)\n",
    "\n",
    "print(\"Alpha as computed based on the given probabilities:\", dir.alpha_mle(model_probs, expert_probs))  ## very high alpha as expected since this is a fixed example\n",
    "print(\"Log likelihood when alpha is not given (computed internally):\", dir.sum_llik(model_probs, expert_probs))\n",
    "\n",
    "\n",
    "dir_2 = Dirichlet(10, J=3)\n",
    "\n",
    "print(\"Log likelihood when alpha = 10 (fixed):\", dir_2.sum_llik(model_probs, expert_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Testing PyMC compatibility with the code by sampling from the height growth model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 8928\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "taus = np.array([0, 2.5, 10, 17.5]) ## the four different ages (J=4)\n",
    "Y = np.array([50, 93, 141, 178])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arpit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [Y_obs, b, h1, htstar, s0, s1, tstar]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.711594\n"
     ]
    }
   ],
   "source": [
    "height_growth_model = pm.Model()\n",
    "\n",
    "with height_growth_model:\n",
    "\n",
    "    b = pm.Gamma(\"b\", alpha=27.0535681555964, beta=1.46878708677914)\n",
    "\n",
    "    h1 = pm.LogNormal(\"h1\", mu=5.17689015771121, sigma=0.0108333588483753)\n",
    "    htstar = pm.LogNormal(\"htstar\", mu=5.00257693664254, sigma=0.00917685343012148)\n",
    "    tstar = pm.LogNormal(\"tstar\", mu=2.4248051249855, sigma=0.0408597528683811)\n",
    "    s0 = pm.LogNormal(\"s0\", mu=-2.62416052915936, sigma=0.0607241107175087)\n",
    "    s1 = pm.LogNormal(\"s1\", mu=0.991030985256659, sigma=1.021159164088)\n",
    "\n",
    "    h = h1 - 2*(h1 - htstar)/(np.exp(s0*(taus - tstar)) + np.exp(s1*(taus - tstar)))\n",
    "\n",
    "    Y_obs = pm.Weibull(\"Y_obs\", alpha=b, beta=h, observed=Y)  ### In the paper the parameterization was done using mean and variance, while in PyMC it is with scale and shape.\n",
    "                                                              ### I am not sure about the correctness of the Weibull initialization here, but the sampled values are realistic and this is just a trial run.\n",
    "\n",
    "\n",
    "with height_growth_model:\n",
    "    idata = pm.sample_prior_predictive(random_seed=RANDOM_SEED)\n",
    "\n",
    "\n",
    "### Expert probabilities (Height growth model, Arpit judgements)\n",
    "\n",
    "probs_1 = np.array([[20, 40, 46, 50, 54, 58], [40, 46, 50, 54, 58, 87], [0.1, 0.15, 0.25, 0.25, 0.15, 0.1]]).T\n",
    "probs_2 = np.array([[30, 60, 65, 68, 72, 75], [60, 65, 68, 72, 75, 112.5], [0.1, 0.15, 0.25, 0.25, 0.15, 0.1]]).T\n",
    "probs_3 = np.array([[57.5, 115, 118, 122, 126, 128], [115, 118, 122, 126, 128, 192], [0.1, 0.15, 0.25, 0.25, 0.15, 0.1]]).T\n",
    "probs_4 = np.array([[77.5, 155, 162, 170, 180, 190], [155, 162, 170, 180, 190, 285], [0.1, 0.15, 0.25, 0.25, 0.15, 0.1]]).T\n",
    "\n",
    "probs = [probs_1, probs_2, probs_3, probs_4]\n",
    "\n",
    "prob_class = PPEProbabilities(target_type=\"continuous\", path=False)\n",
    "\n",
    "\n",
    "partitions, expert_probs = prob_class.get_expert_data(probs)\n",
    "\n",
    "\n",
    "\n",
    "samples_1 = idata.prior_predictive[\"Y_obs\"][0][:,0]\n",
    "samples_2 = idata.prior_predictive[\"Y_obs\"][0][:,1]\n",
    "samples_3 = idata.prior_predictive[\"Y_obs\"][0][:,2]\n",
    "samples_4 = idata.prior_predictive[\"Y_obs\"][0][:,3]\n",
    "\n",
    "samples = np.vstack((samples_1, samples_2, samples_3, samples_4)).T\n",
    "\n",
    "\n",
    "model_probs = prob_class.ppd_probs(samples, partitions)\n",
    "\n",
    "dir = Dirichlet(None, J=4)\n",
    "\n",
    "print(dir.alpha_mle(model_probs, expert_probs)) ## alpha ~ 10.7, suggesting that the hyperparameter vector lambda leads to decent results! (in the R implementation it was ~9.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [Y_obs, b, h1, htstar, s0, s1, tstar]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.116243\n"
     ]
    }
   ],
   "source": [
    "height_growth_model = pm.Model()\n",
    "\n",
    "with height_growth_model:\n",
    "\n",
    "    b = pm.Gamma(\"b\", alpha=2.70001732310141, beta=0.0649993151356405)\n",
    "\n",
    "    h1 = pm.LogNormal(\"h1\", mu=5.21564896751713, sigma=0.0178291680142866)\n",
    "    htstar = pm.LogNormal(\"htstar\", mu=5.03276720905197, sigma=0.00491627071461396)\n",
    "    tstar = pm.LogNormal(\"tstar\", mu=2.69376845516994, sigma=0.151047140645826)\n",
    "    s0 = pm.LogNormal(\"s0\", mu=-2.92277538593031, sigma=0.0986789610518291)\n",
    "    s1 = pm.LogNormal(\"s1\", mu=5.03309751276888, sigma=0.440893081072111)\n",
    "\n",
    "    h = h1 - 2*(h1 - htstar)/(np.exp(s0*(taus - tstar)) + np.exp(s1*(taus - tstar)))\n",
    "\n",
    "    Y_obs = pm.Weibull(\"Y_obs\", alpha=b, beta=h, observed=Y)\n",
    "\n",
    "with height_growth_model:\n",
    "    idata = pm.sample_prior_predictive(random_seed=RANDOM_SEED)\n",
    "\n",
    "\n",
    "probs_1 = np.array([[22.5, 45, 48, 51, 54, 55], [45, 48, 51, 54, 55, 82.5], [0.1, 0.15, 0.25, 0.25, 0.15, 0.1]]).T\n",
    "probs_2 = np.array([[27.5, 55, 60, 65, 67, 69], [55, 60, 65, 67, 69, 103.5], [0.1, 0.15, 0.25, 0.25, 0.15, 0.1]]).T\n",
    "probs_3 = np.array([[57.5, 90, 95, 100, 105, 110], [90, 95, 100, 105, 110, 165], [0.1, 0.15, 0.25, 0.25, 0.15, 0.1]]).T\n",
    "probs_4 = np.array([[77.5, 160, 170, 177, 185, 190], [160, 170, 177, 185, 190, 285], [0.1, 0.15, 0.25, 0.25, 0.15, 0.1]]).T\n",
    "\n",
    "probs = [probs_1, probs_2, probs_3, probs_4]\n",
    "\n",
    "prob_class = PPEProbabilities(target_type=\"continuous\", path=False)\n",
    "\n",
    "\n",
    "partitions, expert_probs = prob_class.get_expert_data(probs)\n",
    "\n",
    "\n",
    "\n",
    "samples_1 = idata.prior_predictive[\"Y_obs\"][0][:,0]\n",
    "samples_2 = idata.prior_predictive[\"Y_obs\"][0][:,1]\n",
    "samples_3 = idata.prior_predictive[\"Y_obs\"][0][:,2]\n",
    "samples_4 = idata.prior_predictive[\"Y_obs\"][0][:,3]\n",
    "\n",
    "samples = np.vstack((samples_1, samples_2, samples_3, samples_4)).T\n",
    "\n",
    "\n",
    "model_probs = prob_class.ppd_probs(samples, partitions)\n",
    "\n",
    "dir = Dirichlet(None, J=4)\n",
    "\n",
    "print(dir.alpha_mle(model_probs, expert_probs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mikko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [Y_obs, b, h1, htstar, s0, s1, tstar]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.40905\n"
     ]
    }
   ],
   "source": [
    "height_growth_model = pm.Model()\n",
    "\n",
    "with height_growth_model:\n",
    "\n",
    "    b = pm.Gamma(\"b\", alpha=14.5769504031241, beta=1.00803466606972)\n",
    "\n",
    "    h1 = pm.LogNormal(\"h1\", mu=5.16190754, sigma=0.000127077512157917)\n",
    "    htstar = pm.LogNormal(\"htstar\", mu=5.09254598, sigma=0.000939028084889941)\n",
    "    tstar = pm.LogNormal(\"tstar\", mu=2.68699688, sigma=0.06363416)\n",
    "    s0 = pm.LogNormal(\"s0\", mu=-2.1839354, sigma=0.000752964903353101)\n",
    "    s1 = pm.LogNormal(\"s1\", mu=8.2057381, sigma=2.71434539992374)\n",
    "\n",
    "    h = h1 - 2*(h1 - htstar)/(np.exp(s0*(taus - tstar)) + np.exp(s1*(taus - tstar)))\n",
    "\n",
    "    Y_obs = pm.Weibull(\"Y_obs\", alpha=b, beta=h, observed=Y)\n",
    "\n",
    "with height_growth_model:\n",
    "    idata = pm.sample_prior_predictive(random_seed=RANDOM_SEED)\n",
    "\n",
    "\n",
    "probs_1 = np.array([[19, 38, 45, 50, 55, 60], [38, 45, 50, 55, 60, 90], [0.1, 0.15, 0.25, 0.25, 0.15, 0.1]]).T\n",
    "probs_2 = np.array([[35, 70, 80, 90, 93, 97], [70, 80, 90, 93, 97, 145.5], [0.1, 0.15, 0.25, 0.25, 0.15, 0.1]]).T\n",
    "probs_3 = np.array([[57.5, 115, 120, 130, 140, 145], [115, 120, 130, 140, 145, 217.5], [0.1, 0.15, 0.25, 0.25, 0.15, 0.1]]).T\n",
    "probs_4 = np.array([[77.5, 155, 165, 175, 185, 188], [155, 165, 175, 185, 188, 282], [0.1, 0.15, 0.25, 0.25, 0.15, 0.1]]).T\n",
    "\n",
    "probs = [probs_1, probs_2, probs_3, probs_4]\n",
    "\n",
    "prob_class = PPEProbabilities(target_type=\"continuous\", path=False)\n",
    "\n",
    "\n",
    "partitions, expert_probs = prob_class.get_expert_data(probs)\n",
    "\n",
    "\n",
    "\n",
    "samples_1 = idata.prior_predictive[\"Y_obs\"][0][:,0]\n",
    "samples_2 = idata.prior_predictive[\"Y_obs\"][0][:,1]\n",
    "samples_3 = idata.prior_predictive[\"Y_obs\"][0][:,2]\n",
    "samples_4 = idata.prior_predictive[\"Y_obs\"][0][:,3]\n",
    "\n",
    "samples = np.vstack((samples_1, samples_2, samples_3, samples_4)).T\n",
    "\n",
    "\n",
    "model_probs = prob_class.ppd_probs(samples, partitions)\n",
    "\n",
    "dir = Dirichlet(None, J=4)\n",
    "\n",
    "print(dir.alpha_mle(model_probs, expert_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppe3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
