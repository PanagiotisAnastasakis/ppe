# Prior-Predictive-Elicitation
This repository contains methods for eliciting priors, following the paper ['Flexible Prior Elicitation via the Prior Predictive Distribution'](https://proceedings.mlr.press/v124/hartmann20a.html), by Hartmann et al.. The approach presented in the paper eventually comes down to the optimization of the Dirichlet log-likelihood, informed by the prior predictive distribution of the target variable and an expert's input in form of probabilistic statements about the target. If we have the prior predictive distribution available in closed form, we can perform gradient descent to obtain estimates for the hyperparameters that define the priors. If not, we can either rely on gradient-free methods (such as Bayesian Optimization), or we can compute stochastic gradients for prior predictive probabilities, which will be used to obtain the gradient of the log likelihood and perform gradient descent. 

So far, our implementation supports gradient descent when a closed form for the prior predictive probability is available, while we are working on stochastic optimization for more general models. Additionally, our script supports Bayesian Optimization, where a PyMC probabilistic model for the target is given as input.
